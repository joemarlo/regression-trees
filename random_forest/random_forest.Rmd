---
title: "Bottom-up random forest, a tutorial"
author: "Joe Marlo"
date: "4/23/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
wd <- '~/Dropbox/Data/Projects/regression-trees'
```

This is a step-by-step guide to build a random forest classification algorithm in "base" R from the bottom-up. We'll start with the Gini impurity metric then move to decision trees and then ensemble methods. R has many packages that include -- faster and better -- implementations of random forest. The goal of this guide is to walk through the fundamentals. The final algorithm will be functional but it is not robust to bugs or programming edge cases. The goal is to provide you with an accurate mental model of how random forest works and, therefore, intuition to where it will perform well and where it fall short.

## Process

Random forests are an ensemble model consisting of many decision trees. We must first create a decision tree classifier which themselves require an impurity metric and recursive algorithm to split the branches. The steps to build a random forest can broken down to:  

  - Define an impurity metric which drives each split in the decision tree  
  - Program a decision algorithm to choose the best data split based on the impurity measure  
  - Program a decision tree algorithm by recursively calling the decision algorithm  
  - Program a bagging model by implementing many decision trees and resampling the data  
  - Program a random forest model by implementing many decision trees, resampling the data, and sampling from the columns  

## Intuition

Binary decision trees create an interpretable decision making framework for making a single prediction. Suppose a patient comes into your clinic with chest pain and you wish to diagnose them with either a heart attack or not a heart attack. A simple framework of coming to that diagnosis could look like the following. 

<p align="center">
<img src="plots/diagram.png" width=50%>
</p>

Note that each split results in two outcomes (binary) and every possible condition leads to a terminal node. The model's splits can also be visualized has partitioning the parameter space. Since the decision tree makes binary splits along a parameter, the resulting boundaries will always be rectangular.

<p align="center">
<img src="plots/diagram_parameters.png" width=50%>
</p>

But where to split the data? The splits are determined via an impurity index. With each split, the algorithm is trying to maximize the purity of the resulting splits. So if a potential split results in classes `[HA, HA]` and [`NHA`, `NHA`] then that is better than another potential split that results `[HA, NHA]` and `[NHA, HA]`. At each node, all possible splits are tested and the split that maximizes purity is chosen.

For classification problems, a commonly used metric is [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity). Gini impurity is `2 * p * (1 - p)` where `p` is the fraction of elements labeled as the class of interest. A value of `0` is a completely homogeneous vector while `0.5` is the inverse. The vector `[NHA, HA, NHA]` has a Gini value of `2 * 1/3 * 2/3 = 0.444`. Since the data is split into two vectors, the value is weighted by the respective lengths of the two vectors. 


## Setup

Okay, it's not quite "base" R as we're going to use the `tidyverse` meta-package for general data munging and `parallel` for multi-core processing during bagging and the random forest. 

```{r packages}
library(tidyverse)
library(parallel)
options(mc.cores = detectCores())
set.seed(44)
```

```{r include=FALSE}
source(file.path(wd, "helpers/ggplot_settings.R"))
```

## Gini impurity

We're going to build the random forest algorithm starting with the smallest component: the Gini impurity metric. Note that the output of `gini` is constrained to `[0, 0.5]`.

```{r gini_function, fig.height=4, fig.width=8}
gini <- function(p){
  2 * p * (1 - p)
}
p <- seq(0, 1, 0.1)
ggplot(tibble(p, gini(p)), aes(x = p, y = `gini(p)`)) +
  geom_smooth(color = 'grey70', linetype = 'dashed') +
  geom_point(size = 3)
```
```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/gini.svg"), width = 8, height = 4)
```



For convenience, we're going to wrap the `gini` function so we feed it a vector instead of a probability. The probability is calculated from the mean value of the vector. In practice, this vector will be binary and represent classification labels.

```{r gini_vectors}
gini_vector <- function(X){
  # X should be binary 0 1 or TRUE/FALSE
  gini(mean(X, na.rm = TRUE))
}
X1 <- c(0, 1, 0)
gini_vector(X1)
```

And finally we're going to wrap it again so it gives us the weighted Gini of two vectors,

```{r gini_weighted}
gini_weighted <- function(X1, X2){
  # X should be binary 0 1 or TRUE/FALSE
  if (is.null(X1)) return(0)
  if (is.null(X2)) return(1)
  
  prop_x1 <- length(X1) / (length(X1) + length(X2))
  weighted_gini <- (prop_x1*gini_vector(X1)) + ((1-prop_x1)*gini_vector(X2))
  return(weighted_gini)
}
X2 <- c(1, 1, 1)
gini_weighted(X1, X2)
```
## Spliting

lorem ipsum

```{r optimal_split}
optimal_split <- function(X, classes, n_splits = 50){
  
  # create "dividing lines" that split X into to parts
  # a smarter version would account for X's values
  splits <- seq(min(X), max(X), length.out = n_splits)
  
  # calculate gini for each potential split
  gini_index <- sapply(splits, function(split){
    X1 <- classes[X <= split]
    X2 <- classes[X > split]
    gini_index <- gini_weighted(X1, X2)
    return(gini_index)
  })

  # choose the best split based on the minimum (most pure) gini value
  gini_minimum <- min(gini_index, na.rm = TRUE)
  optimal_split <- na.omit(splits[gini_index == gini_minimum])[1]
  
  # best prediction for these data are the means of the classes
  classes_split <- split(classes, X <= optimal_split)
  split0 <- tryCatch(mean(classes_split[[2]], na.rm = TRUE), error = function(e) NULL)
  split1 <- tryCatch(mean(classes_split[[1]], na.rm = TRUE),  error = function(e) NULL)
  preds <- list(split0 = split0, split1 = split1)
  
  return(list(gini = gini_minimum, split_value = optimal_split, preds = preds))
}
X <- rnorm(10)
classes <- rbinom(10, 1, 0.5)
optimal_split(X, classes)
```


```{r optimal_split_plot, fig.height=3, fig.width=8}
# plot the points and the optimal split
tibble(x = X, y = 0, col = classes) %>%
  ggplot(aes(x = x, y = y, color = as.factor(col))) +
  geom_point(size = 7, alpha = 0.8) +
  geom_vline(xintercept = optimal_split(X, classes)$split_value,
             linetype = 'dashed') +
  annotate('text', x = optimal_split(X, classes)$split_value, y = 0,
           label = 'Optimal split', angle = 90, vjust = -1, hjust = -0.05) +
  scale_y_continuous(labels = NULL) +
  labs(color = 'Class',
       y = NULL)
```

```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/optimal_split.svg"), width = 8, height = 3)
```

Best feature to split on


```{r best_feature_to_split}
best_feature_to_split <- function(X, Y){
  # X must be a dataframe, Y a vector of 0:1

  # get optimal split for each column
  ginis <- sapply(X, function(x) optimal_split(x, Y))
  
  # return the the column with best split and its splitting value
  best_gini <- min(unlist(ginis['gini',]))[1]
  best_column <- names(which.min(ginis['gini',]))[1]
  best_split <- ginis[['split_value', best_column]]
  pred <- ginis[['preds', best_column]]
  return(list(column = best_column, gini = best_gini, split = best_split, pred = pred))
}
n <- 1000
.data <- tibble(Y = rbinom(n, 1, prob = 0.3),
                X1 = rnorm(n),
                X2 = rnorm(n),
                X3 = rbinom(n, 1, prob = 0.5))
X <- .data[, -1]
Y <- .data[[1]]
best_feature_to_split(.data[, -1], .data[[1]])
```


## Decision trees

### Recursion

To create the decision trees, we need to apply the spliting algorithm until it reaches a certain stopping threshold. We won't know prior how many splits it is going to make -- the depth or the width. Therefore it is not easily solved using a `while` loop because a split results in two new branches. We need to use [recursion](https://www.cs.utah.edu/~germain/PPS/Topics/recursion.html). 

In recursive functions, the function is called within itself until some stopping criteria is met. It will run indefinitely without the stopping criteria. A simple example is the [quicksort](https://algs4.cs.princeton.edu/23quicksort/) algorithm which sorts a vector of numbers from smallest to greatest. 

Quicksort is a divide-and-conquer method that splits the input vector into two vectors based on a pivot point. Points smaller than the pivot go to one vector, points larger to the other vector. The pivot point can be any point but is often the first or last. The function is called on itself to repeat the splitting until one or less numbers exist in a split. Then these sorted child-vectors are passed upward through the recursed functions. 

See the below example.

```{r quicksort}
quick_sort <- function(X){
  
  # stopping criteria: stop if X is length 1 or less
  if (length(X) <= 1) return(X)
  
  # create the pivot point and remove it from the vector
  pivot_point <- X[1]
  X_vec <- X[-1]
  
  # create the lower and upper vectors
  lower_vec <- X_vec[X_vec <= pivot_point]
  upper_vec <- X_vec[X_vec > pivot_point]
  
  # call the function recursively
  lower_sorted <- quick_sort(lower_vec)
  upper_sorted <- quick_sort(upper_vec)
  
  # return the sorted vector
  return(c(lower_sorted, pivot_point, upper_sorted))
}
X <- rnorm(20)
quick_sort(X)
```

### Recursive branching

We're going to pull together all the above functions into one recursive function that becomes the decision tree classifier. 

TODO: explain outline of function

Stopping criteria:  

- `max_depth`: the depth or number of splits in the tree  
- `min_observations`: the fewest number of observations to include in a split  
- `gini_threshold`: the gini impurity amount of the split. Stops if it falls below this  

```{r decision_tree_classifier}
# create a dataframe denoting the best splits
decision_tree_classifier <- function(X, Y, gini_threshold = 0.4, max_depth = 5, min_observations = 5, branch_id = '0', splits = NULL){
  
  
  # calculate the first optimal split
  first_split <- best_feature_to_split(X, Y)
  
  # save the splits
  if (is.null(splits)) splits <- tibble()
  splits <- bind_rows(splits, tibble(column = first_split$column, 
                                     split = first_split$split,
                                     pred = list(first_split$pred),
                                     branch = branch_id))
  
  # calculate current branch depth 
  branch_depth <- splits %>%
    rowwise() %>%
    filter(branch == str_sub(branch_id, 0, nchar(branch))) %>%
    nrow()

  # create two dataframes based on the first split
  X_split <- split(X, X[first_split$column] >= first_split$split)
  Y_split <- split(Y, X[first_split$column] >= first_split$split)
  
  # stopping criteria
  is_too_deep <- isTRUE(branch_depth >= max_depth)
  is_pure_enough <- isTRUE(first_split$gini <= gini_threshold)
  too_few_observations <- isTRUE(min(sapply(X_split, nrow)) < min_observations)
  if (is_too_deep | is_pure_enough | too_few_observations){
    return(splits)
  } else {

    # continue splitting
    # the try will catch errors due to one split group having no observations
    split0 <- tryCatch({
      decision_tree_classifier(
        X = X_split[[1]],
        Y = Y_split[[1]],
        gini_threshold = gini_threshold,
        max_depth = max_depth,
        min_observations = min_observations,
        branch_id = paste0(branch_id, "0"),
        splits = splits
      )
    }, error = function(e) NULL
    )
    split1 <- tryCatch({
      decision_tree_classifier(
        X = X_split[[2]], 
        Y = Y_split[[2]], 
        gini_threshold = gini_threshold, 
        max_depth = max_depth, 
        min_observations = min_observations, 
        branch_id = paste0(branch_id, "1"),
        splits = splits
      )
    }, error = function(e) NULL
    )
    
    # bind rows into a dataframe and remove duplicates caused by diverging branches
    all_splits <- distinct(bind_rows(split0, split1))
    
    return(all_splits)
  }
}
```


```{r}
n <- 1000
.data <- tibble(Y = rbinom(n, 1, prob = 0.3),
                X1 = rnorm(n),
                X2 = rnorm(n),
                X3 = rbinom(n, 1, prob = 0.5))
X <- .data[, -1]
Y <- .data[[1]]
decision_tree <- decision_tree_classifier(X, Y, 0.1)
decision_tree
```

Predict a new observation In this case, we're only predicting in-sample observations.

```{r predict_data_point}
# predict a new data point
predict_data_point <- function(model_decision_tree, new_row, branch_id = '0'){
  
  # traverse the decision tree and get the next split
  decision_point <- model_decision_tree[model_decision_tree$branch == branch_id,]
  decision_split <- new_row[, decision_point$column] < decision_point$split
  decision_split <- if_else(decision_split, 0, 1) 
  branch_id <- paste0(branch_id, decision_split)
  
  # if the new branch_id (i.e. the next split) is not in the decision tree then return the current prediction
  if (!(branch_id %in% model_decision_tree$branch)){
    pred <- decision_point$pred[[1]][[decision_split + 1]]
    return(pred) 
  } else {
    # otherwise continue recursion
    return(predict_data_point(model_decision_tree, new_row, branch_id))
  }
}
predict_data_point(decision_tree, X[1,])
```

Wrap the `predict_data_point` function in a loop so we can predict all observations in a dataframe.

```{r predict_decision_tree}
# predict all datapoints in a dataframe
predict_decision_tree <- function(model_decision_tree, new_data){
  preds <- rep(NULL, nrow(new_data))
  for (i in 1:nrow(new_data)){
    preds[i] <- predict_data_point(model_decision_tree, new_row = new_data[i,])
  }
  return(preds)
}
preds <- predict_decision_tree(decision_tree, X)
```


Now finally we can test our decision tree classifier on data. Decision trees perform best on data split horizontally and/or vertically -- i.e. on data where linear regression or logistical regression tends to perform poorly. Here we create data in an sideways "L" shape where the bottom right quadrant contains data of Class 1, all other quadrants are Class 0.  

```{r example_data}
# create a two dimensional dataset
n <- 1000
.data <- tibble(X1 = runif(n, -1, 1),
                X2 = runif(n, -1, 1),
                Y = (round(X1) == round(X2)) * 1)

# plot it
.data %>% 
  mutate(Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y)) +
  geom_point(size = 3, alpha = 0.5) +
  labs(shape = NULL)
```
```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/example_data.png"), width = 8, height = 6)
```

Let's try to classify this with a logistic regression.

```{r}
# logistic regression
model_log <- glm(Y ~ X1 + X2, data = .data, family = 'binomial')

# plot it
.data %>% 
  mutate(preds = model_log$fitted.values > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```
```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/logistic_classes.png"), width = 8, height = 6)
```

Logistic regression is performing poorly.  These "square" data suit the decision tree classifier well. Below, the classifier almost correctly classifies all points. The incorrect points all lie around the boundary between Class 0 and Class 1. This may be due to the `splits` in `optimal_split` not being granular enough.

```{r plot_rpart_example}
X <- dplyr::select(.data, -Y)
Y <- .data[['Y']]
model_decision_tree <- decision_tree_classifier(X, Y, max_depth = 4, gini_threshold = 0, min_observations = 1)
preds <- predict_decision_tree(model_decision_tree, X)

# plot it
.data %>% 
  mutate(preds = preds > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```
```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/tree_classes.png"), width = 8, height = 6)
```


We can visualize the splits made the decision tree classifier. As expected, these splits are right along the rectangle boundaries.

```{r}
splits_X1 <- split(model_decision_tree, model_decision_tree$column)[[1]]
splits_X2 <- split(model_decision_tree, model_decision_tree$column)[[2]]

.data %>% 
  mutate(preds = preds > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_vline(data = splits_X1, aes(xintercept = split), color = 'blue') +
  geom_hline(data = splits_X2, aes(yintercept = split), color = 'blue') +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```
```{r include=FALSE}
ggsave(file.path(wd, "random_forest/plots/tree_boundaries.png"), width = 8, height = 6)
```

### Where trees can struggle

Trees will struggle when the parameter space is dissected at an angle by the classification value. Regression trees are partitioning the parameter space into rectangles. Any time the the classification value is not rectangular, the regression tree will need to be deeper to estimate the decision boundary.

The below data's classification is in two separate triangles: top left and bottom right of the plot. A logistic regression finds the boundary easily.

```{r}
# create a two dimensional dataset
n <- 1000
.data <- tibble(X1 = runif(n, 0, 1),
                X2 = runif(n, 0, 1),
                Y = (X1 > X2) * 1)

# logistic regression
model_log <- glm(Y ~ X1 + X2, data = .data, family = 'binomial')

# plot it
.data %>% 
  mutate(preds = model_log$fitted.values > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```

A regression tree has a difficult time finding the decision boundary.

```{r plot_rpart_example_two}
X <- dplyr::select(.data, -Y)
Y <- .data[['Y']]
model_decision_tree <- decision_tree_classifier(X, Y, max_depth = 2, gini_threshold = 0, min_observations = 1)
preds <- predict_decision_tree(model_decision_tree, X)

# plot it
.data %>% 
  mutate(preds = preds > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```

The six rectangles the decision tree fit are not enough to estimate the angled boundary.

```{r splits_two}
splits_X1 <- split(model_decision_tree, model_decision_tree$column)[[1]]
splits_X2 <- split(model_decision_tree, model_decision_tree$column)[[2]]

# plot it
.data %>% 
  mutate(preds = preds > 0.5, 
         correct = if_else(preds == Y, "Correct classification", "Incorrect"),
         Y = paste0("Class ", Y)) %>% 
  ggplot(aes(x = X1, y = X2, shape = Y, color = correct)) +
  geom_vline(data = splits_X1, aes(xintercept = split), color = 'blue') +
  geom_hline(data = splits_X2, aes(yintercept = split), color = 'blue') +
  geom_point(size = 3, alpha = 0.5) +
  scale_color_manual(values = c('#5f7d71', 'red')) +
  labs(color = NULL,
       shape = NULL)
```

## Bagging

Single decision trees are prone to overfiting and can have high variance on new data. A simple solution is to create many decision trees based on resamples of the data and allow each tree to "vote" on the final classification. This is bagging.

The "vote" from each tree is their prediction for a given observations. The votes are averaged across all the trees and the final classification is determined from this average. The trees are trained on [bootstrapped data](https://online.stat.psu.edu/stat555/node/119/) -- taking repeated samples of the training data with replacement.

```{r bag_it}
bag_it <- function(X_train, Y_train, X_test, n_trees = 100, max_depth = 5, gini_threshold = 0.2, min_observations = 5){
  
  # grow the trees
  preds <- parallel::mclapply(1:n_trees, function(i){
    
    preds <- tryCatch({
      # bootstrap the data
      sample_indices <- sample(1:nrow(X_train), size = nrow(X_train), replace = TRUE)
      X_sampled <- X_train[sample_indices,]
      Y_sampled <- Y_train[sample_indices]
      
      # fit model on subset and then make predictions on all data
      model_decision_tree <- decision_tree_classifier(X_sampled, Y_sampled, max_depth = max_depth, gini_threshold = gini_threshold, min_observations = min_observations)
      preds <- predict_decision_tree(model_decision_tree, X_test)
    },
    error = function(e) NA
    )
    
    return(tibble(preds))
  }) %>% bind_cols()
  
  # average the predictions across each model's prediction
  # this is how each tree "votes"
  preds <- rowMeans(preds, na.rm = TRUE)
  return(preds)
}
```

```{r}
# read in the credit data
credit <- read_csv(file.path(wd, "data/credit_card.csv"))
X <- select(credit, -Class)
Y <- credit$Class

# create train test split
indices <- sample(c(TRUE, FALSE), size = nrow(credit), replace = TRUE, prob = c(0.5, 0.5))
X_train <- X[indices,]
X_test <- X[!indices,]
Y_train <- Y[indices]
Y_test <- Y[!indices]
```
```{r}
# fit the bagged model
preds <- bag_it(X_train, Y_train, X_test, n_trees = 50, max_depth = 10, gini_threshold = 0, min_observations = 5)
table(preds > 0.5, Y_test)
```


## Random forest

Random forest is exactly like bagging except that in addition to bootstrapping the observations, you also take a random subset of the features. Usually the number of features selected is the sqrt of the total number of features.

```{r random_forest}
random_forest <- function(X_train, Y_train, X_test, n_trees = 100, max_depth = 5, gini_threshold = 0.2, min_observations = 5){
  
  # grow the trees
  preds <- parallel::mclapply(1:n_trees, function(i){
    
    preds <- tryCatch({
      # bootstrap the data
      sample_indices <- sample(1:nrow(X_train), size = nrow(X_train), replace = TRUE)
      sample_columns <- sample(1:ncol(X_train), size = ceiling(sqrt(ncol(X_train))), replace = FALSE)
      X_sampled <- X_train[sample_indices, sample_columns]
      Y_sampled <- Y_train[sample_indices]
      
      # fit model on subset and then make predictions on all data
      model_decision_tree <- decision_tree_classifier(
        X_sampled, 
        Y_sampled, 
        max_depth = max_depth, 
        gini_threshold = gini_threshold, 
        min_observations = min_observations
      )
      preds <- predict_decision_tree(model_decision_tree, X_test)
    },
    error = function(e) NA
    )
    
    return(tibble(preds))
  }) %>% bind_cols()
  
  # average the predictions across each model's prediction
  # this is how each tree "votes"
  preds <- rowMeans(preds, na.rm = TRUE)
  return(preds)
}
```

```{r}
# fit the random forest
preds <- random_forest(X_train, Y_train, X_test, n_trees = 50, max_depth = 10, gini_threshold = 0, min_observations = 5)
table(preds > 0.5, Y_test)
```

Compare this against the `ranger` package.

```{r}
model_ranger <- ranger::ranger(Class ~ ., data = credit[indices,], num.trees = 50, max.depth = 10)
preds <- predict(model_ranger, data = credit[!indices,])$predictions
table(preds > 0.5, credit$Class[!indices])
```
Not bad results!


## Conclusion

Benefits of tree methods:  

  - Single trees are easy to explain and interpret  
  - Allows for both classification and regression  
    - For regression, replace the Gini impurity measure with variance of the outcome variable  
  - Allows for binary, continuous, and categorical (with dummy coding) variables  
  - Can handle missing data (if data is not used within a branch)  
  - Ensemble methods are computationally parallelizable  
  
Downsides:  

  - Single trees are easy to overfit  
  - Single trees have high variance  
